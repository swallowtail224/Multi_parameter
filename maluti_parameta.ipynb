{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Embedding, LSTM, Dropout, concatenate\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import plot_model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "#ツイートのテキスト読み込み\n",
    "test = open(\"extract_tweet.txt\", \"r\", encoding=\"utf-8\")\n",
    "lines = test.readlines()\n",
    "test.close()\n",
    "print(len(lines))\n",
    "\n",
    "#ラベル読み込み\n",
    "test = open(\"label.txt\", \"r\", encoding=\"utf-8\")\n",
    "label = test.readlines()\n",
    "test.close()\n",
    "print(len(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "#userID\n",
    "test = open(\"user.txt\",\"r\", encoding=\"utf-8\")\n",
    "uID = test.readlines()\n",
    "test.close()\n",
    "print(len(uID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = []\n",
    "for i in uID:\n",
    "    if i in id_list:\n",
    "        continue\n",
    "    else:\n",
    "        id_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "print(len(id_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_user = []\n",
    "for i in uID:\n",
    "    for j in range(len(id_list)):\n",
    "        if i == id_list[j]:\n",
    "            post_user.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_postUser = np.array(post_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18881 unique tokens.\n",
      "Shape of data tensor:(10000, 50)\n",
      "Shape of label tensor:(10000, 2)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "maxlen = 50\n",
    "training_samples = 8000 # training data 80 : validation data 20\n",
    "validation_samples = 1000\n",
    "test_samples = len(lines) - (training_samples) #+ validation_samples)\n",
    "max_words = 20000\n",
    "\n",
    "# word indexを作成\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(\"Found {} unique tokens.\".format(len(word_index)))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "# バイナリの行列に変換\n",
    "categorical_labels = to_categorical(label)\n",
    "labels = np.asarray(categorical_labels)\n",
    "\n",
    "print(\"Shape of data tensor:{}\".format(data.shape))\n",
    "print(\"Shape of label tensor:{}\".format(labels.shape))\n",
    "\n",
    "#学習データとテストデータに分割\n",
    "x1_test = data[training_samples + validation_samples: training_samples + validation_samples+test_samples]\n",
    "x2_test = n_postUser[training_samples + validation_samples: training_samples + validation_samples+test_samples]\n",
    "y_test = labels[training_samples + validation_samples: training_samples + validation_samples+test_samples]\n",
    "data = data[:training_samples + validation_samples]\n",
    "labels = labels[:training_samples + validation_samples]\n",
    "n_postUser = n_postUser[:training_samples + validation_samples]\n",
    "\n",
    "\n",
    "# 行列をランダムにシャッフルする\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "users = n_postUser[indices]\n",
    "\n",
    "x1_train = data[:training_samples]\n",
    "x2_train = users[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x1_val = data[training_samples: training_samples + validation_samples]\n",
    "x2_val = users[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from functools import partial\n",
    "\n",
    "def normalize_y_pred(y_pred):\n",
    "    return K.one_hot(K.argmax(y_pred), y_pred.shape[-1])\n",
    "\n",
    "def class_true_positive(class_label, y_true, y_pred):\n",
    "    y_pred = normalize_y_pred(y_pred)\n",
    "    return K.cast(K.equal(y_true[:, class_label] + y_pred[:, class_label], 2), K.floatx())\n",
    "\n",
    "def class_accuracy(class_label, y_true, y_pred):\n",
    "    y_pred = normalize_y_pred(y_pred)\n",
    "    return K.cast(K.equal(y_true[:, class_label], y_pred[:, class_label]),\n",
    "                  K.floatx())\n",
    "\n",
    "def class_precision(class_label, y_true, y_pred):\n",
    "    y_pred = normalize_y_pred(y_pred)\n",
    "    return K.sum(class_true_positive(class_label, y_true, y_pred)) / (K.sum(y_pred[:, class_label]) + K.epsilon())\n",
    "\n",
    "\n",
    "def class_recall(class_label, y_true, y_pred):\n",
    "    return K.sum(class_true_positive(class_label, y_true, y_pred)) / (K.sum(y_true[:, class_label]) + K.epsilon())\n",
    "\n",
    "\n",
    "def class_f_measure(class_label, y_true, y_pred):\n",
    "    precision = class_precision(class_label, y_true, y_pred)\n",
    "    recall = class_recall(class_label, y_true, y_pred)\n",
    "    return (2 * precision * recall) / (precision + recall + K.epsilon())\n",
    "\n",
    "\n",
    "def true_positive(y_true, y_pred):\n",
    "    y_pred = normalize_y_pred(y_pred)\n",
    "    return K.cast(K.equal(y_true + y_pred, 2),\n",
    "                  K.floatx())\n",
    "\n",
    "\n",
    "def micro_precision(y_true, y_pred):\n",
    "    y_pred = normalize_y_pred(y_pred)\n",
    "    return K.sum(true_positive(y_true, y_pred)) / (K.sum(y_pred) + K.epsilon())\n",
    "\n",
    "\n",
    "def micro_recall(y_true, y_pred):\n",
    "    return K.sum(true_positive(y_true, y_pred)) / (K.sum(y_true) + K.epsilon())\n",
    "\n",
    "\n",
    "def micro_f_measure(y_true, y_pred):\n",
    "    precision = micro_precision(y_true, y_pred)\n",
    "    recall = micro_recall(y_true, y_pred)\n",
    "    return (2 * precision * recall) / (precision + recall + K.epsilon())\n",
    "\n",
    "\n",
    "def average_accuracy(y_true, y_pred):\n",
    "    class_count = y_pred.shape[-1]\n",
    "    class_acc_list = [class_accuracy(i, y_true, y_pred) for i in range(class_count)]\n",
    "    class_acc_matrix = K.concatenate(class_acc_list, axis=0)\n",
    "    return K.mean(class_acc_matrix, axis=0)\n",
    "\n",
    "\n",
    "def macro_precision(y_true, y_pred):\n",
    "    class_count = y_pred.shape[-1]\n",
    "    return K.sum([class_precision(i, y_true, y_pred) for i in range(class_count)]) / K.cast(class_count, K.floatx())\n",
    "\n",
    "\n",
    "def macro_recall(y_true, y_pred):\n",
    "    class_count = y_pred.shape[-1]\n",
    "    return K.sum([class_recall(i, y_true, y_pred) for i in range(class_count)]) / K.cast(class_count, K.floatx())\n",
    "\n",
    "\n",
    "def macro_f_measure(y_true, y_pred):\n",
    "    precision = macro_precision(y_true, y_pred)\n",
    "    recall = macro_recall(y_true, y_pred)\n",
    "    return (2 * precision * recall) / (precision + recall + K.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_postText (InputLayer)     (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_userID (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 51)           0           input_postText[0][0]             \n",
      "                                                                 input_userID[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 51, 50)       1000000     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 51, 50)       0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 32)           10624       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 32)           0           lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            66          dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,010,690\n",
      "Trainable params: 1,010,690\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "p_input = Input(shape=(50, ), dtype='int32', name='input_postText')\n",
    "u_input = Input(shape=(1, ), dtype='int32', name='input_userID')\n",
    "\n",
    "\n",
    "#em = Embedding(input_dim=20000, output_dim=1024, input_length=50)(p_input)\n",
    "#shered_lstm = LSTM(32)\n",
    "#p_lstm = shered_lstm(em)\n",
    "#u_lstm = shered_lstm(u_input)\n",
    "\n",
    "x = concatenate([p_input, u_input])\n",
    "em = Embedding(input_dim=20000, output_dim=50, input_length=51)(x)\n",
    "d_em = Dropout(0.5)(em)\n",
    "lstm_out = LSTM(32)(d_em)\n",
    "d_lstm_out = Dropout(0.5)(lstm_out)\n",
    "output = Dense(2, activation='softmax', name = 'output')(d_lstm_out)\n",
    "\n",
    "model = Model(inputs=[p_input, u_input], outputs = output)\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy',  metrics=['acc', macro_precision, macro_recall, macro_f_measure])\n",
    "model.summary()\n",
    "#plot_model(model, show_shapes=True, show_layer_names=True, to_file='MultiParameter_model.png')\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(patience=0, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 1000 samples\n",
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 7s 917us/step - loss: 0.5922 - acc: 0.7143 - macro_precision: 0.7394 - macro_recall: 0.7113 - macro_f_measure: 0.7246 - val_loss: 0.4627 - val_acc: 0.8060 - val_macro_precision: 0.8154 - val_macro_recall: 0.8025 - val_macro_f_measure: 0.8089\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 5s 687us/step - loss: 0.3629 - acc: 0.8527 - macro_precision: 0.8568 - macro_recall: 0.8510 - macro_f_measure: 0.8539 - val_loss: 0.4107 - val_acc: 0.8240 - val_macro_precision: 0.8289 - val_macro_recall: 0.8207 - val_macro_f_measure: 0.8248\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 5s 680us/step - loss: 0.2582 - acc: 0.9011 - macro_precision: 0.9022 - macro_recall: 0.9002 - macro_f_measure: 0.9012 - val_loss: 0.4286 - val_acc: 0.8210 - val_macro_precision: 0.8240 - val_macro_recall: 0.8186 - val_macro_f_measure: 0.8213\n",
      "Epoch 00003: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([x1_train, x2_train], y_train,\n",
    "                    epochs=100, \n",
    "                    batch_size=100,\n",
    "                    validation_data=([x1_val, x2_val], y_val),\n",
    "                    callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 1s 773us/step\n",
      "[0.6733322334289551, 0.715, 0.7067137284278869, 0.6364665303230286, 0.6575019345283508]\n"
     ]
    }
   ],
   "source": [
    "loss_and_metrics = model.evaluate([x1_test, x2_test], y_test)\n",
    "print(loss_and_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = model.predict([x1_test, x2_test])\n",
    "np.savetxt('Mult_predict.csv', classes, delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'acc', 'macro_precision', 'macro_recall', 'macro_f_measure']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
