{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Embedding, LSTM, Dropout, concatenate\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import plot_model\n",
    "import numpy as np\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import keras.backend as K\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_y_pred(y_pred):\n",
    "    return K.one_hot(K.argmax(y_pred), y_pred.shape[-1])\n",
    "\n",
    "def class_true_positive(class_label, y_true, y_pred):\n",
    "    y_pred = normalize_y_pred(y_pred)\n",
    "    return K.cast(K.equal(y_true[:, class_label] + y_pred[:, class_label], 2), K.floatx())\n",
    "\n",
    "def class_accuracy(class_label, y_true, y_pred):\n",
    "    y_pred = normalize_y_pred(y_pred)\n",
    "    return K.cast(K.equal(y_true[:, class_label], y_pred[:, class_label]),\n",
    "                  K.floatx())\n",
    "\n",
    "def class_precision(class_label, y_true, y_pred):\n",
    "    y_pred = normalize_y_pred(y_pred)\n",
    "    return K.sum(class_true_positive(class_label, y_true, y_pred)) / (K.sum(y_pred[:, class_label]) + K.epsilon())\n",
    "\n",
    "\n",
    "def class_recall(class_label, y_true, y_pred):\n",
    "    return K.sum(class_true_positive(class_label, y_true, y_pred)) / (K.sum(y_true[:, class_label]) + K.epsilon())\n",
    "\n",
    "\n",
    "def class_f_measure(class_label, y_true, y_pred):\n",
    "    precision = class_precision(class_label, y_true, y_pred)\n",
    "    recall = class_recall(class_label, y_true, y_pred)\n",
    "    return (2 * precision * recall) / (precision + recall + K.epsilon())\n",
    "\n",
    "\n",
    "def true_positive(y_true, y_pred):\n",
    "    y_pred = normalize_y_pred(y_pred)\n",
    "    return K.cast(K.equal(y_true + y_pred, 2),\n",
    "                  K.floatx())\n",
    "\n",
    "\n",
    "def micro_precision(y_true, y_pred):\n",
    "    y_pred = normalize_y_pred(y_pred)\n",
    "    return K.sum(true_positive(y_true, y_pred)) / (K.sum(y_pred) + K.epsilon())\n",
    "\n",
    "\n",
    "def micro_recall(y_true, y_pred):\n",
    "    return K.sum(true_positive(y_true, y_pred)) / (K.sum(y_true) + K.epsilon())\n",
    "\n",
    "\n",
    "def micro_f_measure(y_true, y_pred):\n",
    "    precision = micro_precision(y_true, y_pred)\n",
    "    recall = micro_recall(y_true, y_pred)\n",
    "    return (2 * precision * recall) / (precision + recall + K.epsilon())\n",
    "\n",
    "\n",
    "def average_accuracy(y_true, y_pred):\n",
    "    class_count = y_pred.shape[-1]\n",
    "    class_acc_list = [class_accuracy(i, y_true, y_pred) for i in range(class_count)]\n",
    "    class_acc_matrix = K.concatenate(class_acc_list, axis=0)\n",
    "    return K.mean(class_acc_matrix, axis=0)\n",
    "\n",
    "\n",
    "def macro_precision(y_true, y_pred):\n",
    "    class_count = y_pred.shape[-1]\n",
    "    return K.sum([class_precision(i, y_true, y_pred) for i in range(class_count)]) / K.cast(class_count, K.floatx())\n",
    "\n",
    "\n",
    "def macro_recall(y_true, y_pred):\n",
    "    class_count = y_pred.shape[-1]\n",
    "    return K.sum([class_recall(i, y_true, y_pred) for i in range(class_count)]) / K.cast(class_count, K.floatx())\n",
    "\n",
    "\n",
    "def macro_f_measure(y_true, y_pred):\n",
    "    precision = macro_precision(y_true, y_pred)\n",
    "    recall = macro_recall(y_true, y_pred)\n",
    "    return (2 * precision * recall) / (precision + recall + K.epsilon())\n",
    "\n",
    "def weight_variable(shape):\n",
    "    return K.truncated_normal(shape, stddev = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "#データの読み込み\n",
    "use_data = pd.read_csv(filepath_or_buffer=\"multi_data.csv\", encoding=\"utf_8\", sep=\",\")\n",
    "print(len(use_data))\n",
    "use_data.info()\n",
    "#NaNデータの0埋め\n",
    "use_data = use_data.fillna('0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lines' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e7c895148093>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtraining_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8000\u001b[0m \u001b[0;31m# training data 80 : validation data 20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mvalidation_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtest_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtraining_samples\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvalidation_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mmax_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lines' is not defined"
     ]
    }
   ],
   "source": [
    "maxlen = 50\n",
    "tag_maxlen = 10\n",
    "train = 0.7\n",
    "validation = 0.1\n",
    "max_words = 20000\n",
    "\n",
    "#データをランダムにシャッフル\n",
    "use_data_s = use_data.sample(frac=1, random_state=150)\n",
    "\n",
    "# word indexを作成\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(use_data_s['tweet2'])\n",
    "tokenizer.fit_on_texts(use_data_s['tag2'])\n",
    "sequences = tokenizer.texts_to_sequences(use_data_s['tweet2'])\n",
    "sequences2 = tokenizer.texts_to_sequences(use_data_s['tag2'])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(\"Found {} unique tokens.\".format(len(word_index)))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "t_data =  pad_sequences(sequences2, maxlen=tag_maxlen)\n",
    "\n",
    "#sin,cosデータをまとめてnp行列へ\n",
    "date = use_data_s[['cos_day','sin_day']]\n",
    "p_date = date.values\n",
    "\n",
    "#ラベルをバイナリの行列に変換\n",
    "categorical_labels = to_categorical(use_data_s['retweet'])\n",
    "labels = np.asarray(categorical_labels)\n",
    "\n",
    "print(\"Shape of data tensor:{}\".format(data.shape))\n",
    "print(\"Shape of label tensor:{}\".format(t_data.shape))\n",
    "print(\"Shape of label tensor:{}\".format(p_date.shape))\n",
    "print(\"Shape of label tensor:{}\".format(labels.shape))\n",
    "\n",
    "\n",
    "indices = [int(len(labels) * n) for n in [train, train + validation]]\n",
    "x1_train, x1_val, x1_test = np.split(data, indices)\n",
    "x2_train, x2_val, x2_test = np.split(t_data, indices)\n",
    "x3_train, x3_val, x3_test = np.split(p_date, indices)\n",
    "y_train, y_val, y_test = np.split(labels, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input_postText (InputLayer)     (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Input_tag (InputLayer)          (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "merge1 (Concatenate)            (None, 60)           0           Input_postText[0][0]             \n",
      "                                                                 Input_tag[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Embedding (Embedding)           (None, 60, 60)       1200000     merge1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Input_others (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "LSTM (LSTM)                     (None, 32)           11904       Embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dence1 (Dense)                  (None, 16)           32          Input_others[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "merge2 (Concatenate)            (None, 48)           0           LSTM[0][0]                       \n",
      "                                                                 dence1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dence (Dense)                   (None, 16)           784         merge2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            34          dence[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 1,212,754\n",
      "Trainable params: 1,212,754\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "p_input = Input(shape=(50, ), dtype='int32', name='Input_postText')\n",
    "t_input = Input(shape=(10, ), dtype='int32', name='Input_tag')\n",
    "o_input = Input(shape=(2,), name='Input_others')\n",
    "\n",
    "#テキストとタグの学習\n",
    "x = concatenate([p_input, t_input], name='merge1')\n",
    "em = Embedding(input_dim=20620, output_dim=60, input_length=60, name='Embedding')(x)\n",
    "d_em = Dropout(0.5)(em)\n",
    "lstm_out = LSTM(32, kernel_initializer=weight_variable, name='LSTM')(d_em)\n",
    "d_lstm_out = Dropout(0.5)(lstm_out)\n",
    "\n",
    "#3つ目のデータ学習\n",
    "i3 = Dense(16, activation='relu', name='dence1')(o_input)\n",
    "d_i3 = Dropout(0.5)(i3)\n",
    "\n",
    "x2 = concatenate([d_lstm_out, d_i3], name='merge2')\n",
    "m2 = Dense(16, activation='relu', name = 'dence')(x2)\n",
    "d_m2 = Dropout(0.5)(m2)\n",
    "output = Dense(2, activation='softmax', name = 'output')(d_m2)\n",
    "\n",
    "model = Model(inputs=[p_input, t_input, o_input], outputs = output)\n",
    "optimizer = Adam(lr=1e-3)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy',  metrics=['acc', macro_precision, macro_recall, macro_f_measure])\n",
    "model.summary()\n",
    "#plot_model(model, show_shapes=True, show_layer_names=True, to_file='model_image/model5.png')\n",
    "\n",
    "early_stopping = EarlyStopping(patience=0, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 1000 samples\n",
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 0.6003 - acc: 0.7268 - macro_precision: 0.7489 - macro_recall: 0.7222 - macro_f_measure: 0.7350 - val_loss: 0.4566 - val_acc: 0.8110 - val_macro_precision: 0.8335 - val_macro_recall: 0.8022 - val_macro_f_measure: 0.8175\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 6s 751us/step - loss: 0.3621 - acc: 0.8555 - macro_precision: 0.8587 - macro_recall: 0.8539 - macro_f_measure: 0.8562 - val_loss: 0.4077 - val_acc: 0.8250 - val_macro_precision: 0.8245 - val_macro_recall: 0.8234 - val_macro_f_measure: 0.8239\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 6s 759us/step - loss: 0.2574 - acc: 0.8982 - macro_precision: 0.9005 - macro_recall: 0.8975 - macro_f_measure: 0.8990 - val_loss: 0.4473 - val_acc: 0.8120 - val_macro_precision: 0.8120 - val_macro_recall: 0.8110 - val_macro_f_measure: 0.8115\n",
      "Epoch 00003: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([x1_train, x2_train, x3_train], y_train,\n",
    "                    epochs=100, \n",
    "                    batch_size=256,\n",
    "                    validation_data=([x1_val, x2_val, x3_val], y_val),\n",
    "                    callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 1s 895us/step\n",
      "[0.7138087067604065, 0.735, 0.6996037750244141, 0.6489026327133178, 0.6639105467796326]\n"
     ]
    }
   ],
   "source": [
    "loss_and_metrics = model.evaluate([x1_test, x2_test, x3_test], y_test)\n",
    "print(loss_and_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'acc', 'macro_precision', 'macro_recall', 'macro_f_measure']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = model.predict([x1_test, x2_test, x3_test, x4_test, x5_test])\n",
    "#予測結果を保存して与えたデータと結合\n",
    "columns = ['not publish', 'publish']\n",
    "result = pd.DataFrame(classes, columns = columns)\n",
    "test_data = use_data_s[15998:19999]\n",
    "n_test_data = test_data.reset_index()\n",
    "predict_result = n_test_data.join(result)\n",
    "predict_result.drop(['user_id','tweet_id','tweet2', 'cos_day','sin_day', 'image_url', 'tag2', 'user_id2'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#予測結果の書き出し\n",
    "predict_result.to_csv(\"result/I/result.csv\",index=False, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'b--', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "#plt.savefig(\"result/I/test_and_val_acc.png\")\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'b--', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "#plt.savefig(\"result/I/test_and_val_loss.png\")\n",
    "\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax_acc = fig.add_subplot(111)\n",
    "ax_acc.plot(epochs, val_acc, 'b--', label='Validation acc')\n",
    "plt.legend(bbox_to_anchor=(0, 1), loc='upper left', borderaxespad=0.5, fontsize=10)\n",
    "\n",
    "ax_loss = ax_acc.twinx()\n",
    "ax_loss.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.legend(bbox_to_anchor=(0, 0.9), loc='upper left', borderaxespad=0.5, fontsize=10)\n",
    "plt.title('Validation acc and Validation loss')\n",
    "ax_acc.set_xlabel('epochs')\n",
    "ax_acc.set_ylabel('Validation acc')\n",
    "ax_loss.grid(True)\n",
    "ax_loss.set_ylabel('Validation loss')\n",
    "#plt.savefig(\"result/I/val_acc_loss.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Datas/model5_dI.h5')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "text_representation": {
    "extension": ".py",
    "format_name": "light",
    "format_version": "1.5",
    "jupytext_version": "1.3.1"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
